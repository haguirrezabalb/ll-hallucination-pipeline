# Evaluation results structure

This directory stores the outputs produced by `lm_eval` runs and any associated
post-processing artifacts.

- `baseline/`
  - Pre-QLoRA results.
  - Raw `lm_eval` outputs (JSON), per-sample logs, and aggregated tables.
- `postqlora/`
  - Post-QLoRA results, using exactly the same protocol as the baseline.
- `drafts/`
  - Scratch runs (e.g., `--limit`, experimental parameter changes, etc.).
  - Not used for the final thesis tables.

Naming/layout convention (as produced by the wrapper scripts in `scripts/`):

- `baseline/`
  - `<task>_seed<seed>_full/`
    - `results_*.json`
    - `samples_<task>_*.jsonl`
  - `summary_preqlora_*.csv` (generated by `scripts/summarize_baseline.py`)

- `postqlora/`
  - `postqlora_<task>_seed<seed>/`
    - `results_*.json`
    - `samples_<task>_*.jsonl`
  - `by_adapter/` (symlink view created by `scripts/build_postqlora_by_adapter_links.py`)
  - `summary_postqlora_*.csv` (generated by `scripts/summarize_postqlora.py`)

- `comparison/`
  - `summary_pre_post_aggregated_all_adapters.csv`
  - `paired_bootstrap_truthfulqa_mc2_seed*.csv`
  - `mcnemar_arc_easy_seed*.csv`
  - `delta_summary_for_plots.csv`
