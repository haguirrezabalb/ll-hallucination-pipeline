condition,config_file,dataset_type,dataset_path,max_seq_length,num_train_epochs,max_steps,learning_rate,warmup_ratio,weight_decay,per_device_train_batch_size,gradient_accumulation_steps,effective_batch_size,max_grad_norm,gradient_checkpointing,load_in_4bit,quant_type,compute_dtype,use_double_quant,lora_r,lora_alpha,lora_dropout,bias,target_modules,train_seed,data_seed,model_seed,lora_output_dir
alpaca_1k,qlora_alpaca_1k.toml,hf,yahma/alpaca-cleaned,1024,3,-1,0.0002,0.03,0.0,1,16,16,0.3,True,True,nf4,bfloat16,True,64,16,0.05,none,"q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj",42,42,42,~/models/qwen3-8b-base-qlora-alpaca-1k
alpaca_10k,qlora_alpaca_10k.toml,hf,yahma/alpaca-cleaned,1024,3,-1,0.0002,0.03,0.0,1,16,16,0.3,True,True,nf4,bfloat16,True,64,16,0.05,none,"q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj",42,42,42,~/models/qwen3-8b-base-qlora-alpaca-10k
alpaca_full,qlora_alpaca_full.toml,hf,yahma/alpaca-cleaned,1024,3,-1,0.0002,0.03,0.0,1,16,16,0.3,True,True,nf4,bfloat16,True,64,16,0.05,none,"q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj",42,42,42,~/models/qwen3-8b-base-qlora-alpaca-full
dolly_5k,qlora_dolly_5k.toml,jsonl,docs/datasets/dolly_5k.jsonl,1024,3,-1,0.0002,0.03,0.0,1,16,16,0.3,True,True,nf4,bfloat16,True,64,16,0.05,none,"q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj",42,42,42,~/models/qwen3-8b-base-qlora-dolly-5k
dolly_full,qlora_dolly_full.toml,jsonl,docs/datasets/dolly_5k.jsonl,1024,3,-1,0.0002,0.03,0.0,1,16,16,0.3,True,True,nf4,bfloat16,True,64,16,0.05,none,"q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj",42,42,42,~/models/qwen3-8b-base-qlora-dolly-full
smoketest,qlora_smoketest.toml,jsonl,docs/datasets/qlora_sft_smoketest.jsonl,1024,3,-1,0.0002,0.03,0.0,1,16,16,0.3,True,True,nf4,bfloat16,True,64,16,0.05,none,"q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj",42,42,42,~/models/_smoketests/qwen3-8b-base-qlora-smoketest
